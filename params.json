{"name":"TribeFlow","tagline":"TribeFlow source code","body":"<a name=\"top\"></a>\r\nTribeFlow\r\n=========\r\n\r\n1. [Home](#top)\r\n2. [Datasets](#data)\r\n3. [Competing Methods](#competition)\r\n\r\nContains the TribeFlow (previously node-sherlock) source code.\r\n\r\nDependencies\r\n------------\r\n\r\nThe python dependencies are:\r\n\r\n* Mpi4Py\r\n* numpy\r\n* scipy\r\n* cython\r\n* pandas\r\n* plac\r\n\r\nYou will also need to install and setup: \r\n\r\n* OpenMP\r\n* MPI\r\n\r\nHow to install dependencies\r\n---------------------------\r\n\r\n*Easy way:* Install [Anaconda Python](https://www.continuum.io/) and \r\nset it up as your default enviroment.\r\n\r\n*Hard way:* Use pip or your package manager to install the dependencies. \r\n\r\n```bash\r\npip install numpy\r\npip install scipy\r\npip install cython\r\npip install pandas\r\npip install mpi4py\r\npip install plac\r\n```\r\n\r\nUse or package manager (*apt* on Ubuntu, *HomeBrew* on a mac) to install\r\nOpenMP and MPI. These are the managers I tested with. Should work on any\r\nother environment.\r\n\r\nHow to compile\r\n--------------\r\n\r\nSimply type `make`\r\n\r\n```bash\r\nmake\r\n```\r\n\r\nHow to use\r\n----------\r\n\r\nEither use `python setup.py install` to install the packager or just use it from\r\nthe package folder using the `run_script.sh` command.\r\n\r\n*How to parse datasets:* Use the `scripts/trace_converter.py` script. It has a help.\r\n\r\nFor command line help:\r\n\r\n```bash\r\n$ python scripts/trace_converter.py -h\r\n$ python main.py -h\r\n```\r\n\r\nRunning with mpi\r\n\r\n```bash\r\n$ mpiexec -np 4 python main.py [OPTIONS]\r\n```\r\n\r\nRunning TribeFlow from other python code:\r\n\r\n```\r\nCheck the api_singlecore_example.py file\r\n```\r\n\r\nExample\r\n-------\r\n\r\n**Converting the Trace**\r\n\r\nLet's assume we have a trace like the Last.FM trace from [Oscar\r\nCelma](http://www.dtic.upf.edu/~ocelma/MusicRecommendationDataset/lastfm-1K.html).\r\nIn this example, each line is of the form:\r\n\r\n```bash\r\nuserid \\t timestamp \\t musicbrainz-artist-id \\t artist-name \\t\r\nmusicbrainz-track-id \\t track-name\r\n```\r\n\r\nFor instance:\r\n\r\n```bash\r\nuser_000001 2009-05-01T09:17:36Z    c74ee320-1daa-43e6-89ee-f71070ee9e8f\r\nImpossible Beings   952f360d-d678-40b2-8a64-18b4fa4c5f8Dois PÃ³los\r\n```\r\n\r\nFirst, we want to convert this file to our input format. We do this with the\r\n`scripts/trace_converter.py` script. Let's have a look at the options from\r\nthis script:\r\n\r\n```bash\r\n$ python scripts/trace_converter.py -h\r\nusage: trace_converter.py [-h] [-d DELIMITER] [-l LOOPS] [-r SORT] [-f FMT]\r\n                          [-s SCALE] [-k SKIP_HEADER] [-m MEM_SIZE]\r\n                          original_trace tstamp_column hypernode_column\r\n                          obj_node_column\r\n\r\npositional arguments:\r\n  original_trace        The name of the original trace\r\n  tstamp_column         The column of the time stamp\r\n  hypernode_column      The column of the time hypernode\r\n  obj_node_column       The column of the object node\r\n\r\noptional arguments:\r\n  -h, --help            show this help message and exit\r\n  -d DELIMITER, --delimiter DELIMITER\r\n                        The delimiter\r\n  -l LOOPS, --loops LOOPS\r\n                        Consider loops\r\n  -r SORT, --sort SORT  Sort the trace\r\n  -f FMT, --fmt FMT     The format of the date in the trace\r\n  -s SCALE, --scale SCALE\r\n                        Scale the time by this value\r\n  -k SKIP_HEADER, --skip_header SKIP_HEADER\r\n                        Skip these first k lines\r\n  -m MEM_SIZE, --mem_size MEM_SIZE\r\n                        Memory Size (the markov order is m - 1)\r\n```\r\n\r\nThe positional (obrigatory) arguments are:\r\n\r\n   * *original_trace* is the input file\r\n   * *hypernode_column* represents the users (called hypernodes since it can \r\n     be playlists as well)\r\n   * *tstamp_column* the column of the time stamp\r\n   * *obj_node_column* the objects of interest\r\n\r\nWe can convert the file with the following line:\r\n\r\n```bash\r\npython scripts/trace_converter.py scripts/test_parser.dat 1 0 2 -d$'\\t' \\\r\n        -f'%Y-%m-%dT%H:%M:%SZ' > trace.dat\r\n```\r\n\r\nHere, we are saying that column 1 are the timestamps, 0 is the user, and 2 are the\r\nobjects (artist ids). The delimiter *-d* is a tab. The time stamp format is\r\n`'%Y-%m-%dT%H:%M:%SZ'`.\r\n\r\n***Adding memory***\r\n\r\nUse the -m argument to increase the burst (B parameter in the paper) size. \r\n\r\n```bash\r\npython scripts/trace_converter.py scripts/test_parser.dat 1 0 2 -d$'\\t' \\\r\n        -f'%Y-%m-%dT%H:%M:%SZ' -m 3 > trace.dat\r\n```\r\n\r\n**Learning the Model**\r\n\r\nThe example below is the same code used for every result in the paper. It runs\r\nTribeFlow with the options used in every result in the paper. Explaining the\r\nparameters:\r\n\r\n   * *-np 20* Number of cores for execution.\r\n   * *100* topics.\r\n   * *output.h5* model file.\r\n   * *--kernel eccdf* The kernel heuristic for inter-event time estimation. ECCDF\r\n     based as per described on the paper. We also have a t-student kernel.\r\n   * *--residency_priors 1 99* The priors for the inter-event time estimation.\r\n   * *--leaveout 0.3* Number of transitions to leaveout.\r\n   * *--num_iter 2000* Number of iterations.\r\n   * *--num_batches 20* Number of split/merge moves.\r\n\r\n*The example below uses 20 cores*\r\n```bash\r\n$ mpiexec -np 20 python main.py trace.dat 100 output.h5 \\\r\n    --kernel eccdf --residency_priors 1 99 \\\r\n    --leaveout 0.3 --num_iter 2000 --num_batches 20\r\n```\r\n\r\n<a name=\"data\"></a>\r\nDatasets\r\n========\r\n\r\nBelow we have the list of datasets explored on the paper. We also curated links\r\nto various other timestamp datasets that can be exploited by TribeFlow and \r\nfuture efforts.\r\n\r\nDatasets used on the paper:\r\n\r\n1. [LastFM-1k](http://www.dtic.upf.edu/~ocelma/MusicRecommendationDataset/lastfm-1K.html)\r\n2. *LastFM-Our* Drop me an e-mail for now, looking for a place to upload it.\r\n3. [FourSQ](https://archive.org/details/201309_foursquare_dataset_umn)\r\n    This dataset was removed from the original website. Still available on\r\n    archive. Other, more recent, FourSQ datasets are available. See below.\r\n4. [Brightkite](https://snap.stanford.edu/data/loc-brightkite.html)\r\n5. [Yes](http://www.cs.cornell.edu/people/tj/playlists/index.html)\r\n\r\nList of other, some more recent, datasets that can be explored by TribeFlow.\r\n\r\n1. [Newer FourSQ](https://sites.google.com/site/yangdingqi/home/foursquare-dataset)\r\n2. [Million Music Tweet](http://www.cp.jku.at/datasets/MMTD/)\r\n3. [Movie Ratings](https://github.com/sidooms/MovieTweetings)\r\n4. [Twitter](https://snap.stanford.edu/data/twitter7.html)\r\n5. [Gowalla](https://snap.stanford.edu/data/loc-gowalla.html)\r\n6. [Yelp](https://www.yelp.com/dataset_challenge)\r\n7. [Best Buy](https://www.kaggle.com/c/acm-sf-chapter-hackathon-big/data)\r\n\r\nBasically, anything with users (playlists, actors, etc also work), objects and \r\ntimestamps.\r\n\r\nOn the `example` folder we have some sub-sampled datasets that can be used to\r\nbetter understand the method.\r\n\r\n<a name=\"competition\"></a>\r\nCompeting Methods\r\n=================\r\n\r\n* [HMMs](https://github.com/guyz/HMM) - or any other implementation\r\n* [PRLME](http://github.com/flaviovdf/plme)\r\n* [FPMC](http://github.com/flaviovdf/fpmc)\r\n* [LME](http://www.cs.cornell.edu/people/tj/playlists/index.html)\r\n* [Gravity Model](https://github.com/flaviovdf/tribeflow/blob/master/scripts/gravity_model.py)\r\n* [TMLDA](https://github.com/flaviovdf/tribeflow/blob/master/scripts/tmlda.py)\r\n* [StagesModel](http://infolab.stanford.edu/~crucis/code/stages-package.zip)\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}